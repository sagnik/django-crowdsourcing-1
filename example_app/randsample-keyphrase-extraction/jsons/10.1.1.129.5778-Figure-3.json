{"Caption":"Figure 3. Embeddings from kernel PCA with the Gaussian kernel on the Swiss roll and teapot data sets in Figs. 1 and 2. The first three principal components are shown. In both cases, different patches of the manifolds are mapped to orthogonal parts of feature space. ","ImageText":[{"Text":"C","TextBB":[127.245,105.346,132.652,112.834],"Rotation":0},{"Text":"B","TextBB":[152.205,122.818,157.612,130.306],"Rotation":0},{"Text":"D","TextBB":[107.277,143.41,112.684,150.898],"Rotation":0},{"Text":"A","TextBB":[270.411,126.725,275.606,133.92],"Rotation":0},{"Text":"B","TextBB":[308.182,126.725,313.377,133.92],"Rotation":0},{"Text":"C","TextBB":[345.953,126.725,351.148,133.92],"Rotation":0},{"Text":"D","TextBB":[384.324,126.725,389.519,133.92],"Rotation":0},{"Text":"A","TextBB":[135.981,143.41,141.388,150.898],"Rotation":0},{"Text":"B","TextBB":[184.653,162.754,190.06,170.242],"Rotation":0},{"Text":"A","TextBB":[249.427,159.101,254.622,166.295],"Rotation":0},{"Text":"D","TextBB":[366.338,172.89,371.532,180.085],"Rotation":0},{"Text":"C","TextBB":[319.574,180.085,324.768,187.279],"Rotation":0},{"Text":"C","TextBB":[184.653,193.954,190.06,201.442],"Rotation":0},{"Text":"A","TextBB":[93.5495,206.434,98.9558,213.922],"Rotation":0},{"Text":"D","TextBB":[173.421,268.21,178.828,275.698],"Rotation":0},{"Text":"B","TextBB":[296.191,263.421,301.386,270.616],"Rotation":0}],"Mention":["sult, the different patches of the manifold are mapped\ninto orthogonal regions of the feature space: see Fig. 3.\nThus, rather than unfolding the manifold, the Gaus-\nsian kernel leads to an embedding whose dimensional-\nity is equal to the number of non-overlapping patches\nof length scale Ïƒ. This explains the generally poor per-\nformance of the Gaussian kernel for manifold learning\n(as well as its generally good performance for large\nmargin classification, discussed in section 4.2).\n"],"Page":6,"Number":3,"Type":"Figure","CaptionBB":[75,302,402,376],"Height":1100,"Width":850,"DPI":100,"ImageBB":[77,87,407,281]}