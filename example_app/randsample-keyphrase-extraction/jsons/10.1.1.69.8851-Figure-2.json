{"Caption":"Figure 2: An example Auditory and Visual pattern vector. The figure shows which dimensions went into each of Ax, Ay, Vx, and Vy. ","ImageText":[{"Text":"Vx","TextBB":[502.354,119.551,512.745,127.787],"Rotation":0},{"Text":"time","TextBB":[299.968,128.532,316.787,136.768],"Rotation":0},{"Text":"Vy","TextBB":[502.354,137.539,512.745,145.775],"Rotation":0},{"Text":"motion","TextBB":[521.813,121.022,548.534,129.258],"Rotation":0},{"Text":"vectors","TextBB":[521.814,133.023,550.51,141.259],"Rotation":0},{"Text":"image","TextBB":[380.928,155.527,405.173,163.763],"Rotation":0},{"Text":"areas","TextBB":[407.648,155.527,429.916,163.763],"Rotation":0},{"Text":"Ax","TextBB":[499.361,214.008,509.752,222.244],"Rotation":0},{"Text":"frequency","TextBB":[509.813,224.46,548.909,232.696],"Rotation":0},{"Text":"vectors","TextBB":[511.31,236.461,540.007,244.697],"Rotation":0},{"Text":"time","TextBB":[299.968,237.957,316.787,246.193],"Rotation":0},{"Text":"Ay","TextBB":[499.361,255.972,509.752,264.208],"Rotation":0},{"Text":"frequency","TextBB":[382.425,276.953,421.521,285.189],"Rotation":0},{"Text":"channels","TextBB":[423.996,276.953,459.629,285.189],"Rotation":0}],"Mention":["The original auditory and visual data were collected using an 8mm camcorder and direc-\ntional microphone. The speaker spoke 118 repetitions of \/ba\/, \/va\/, \/da\/, \/ga\/, and \/wa\/. The\nfirst 98 samples of each utterance class formed the training set and the remaining 20 the\ntest set. The auditory feature vector was encoded using a 24 channel mel code1 over 20\nmsec windows overlapped by 10 msec. This is a coarse short time frequency encoding,\nwhich crudely approximates peripheral auditory processing. Each feature vector was lin-\nearly scaled so that all dimensions lie in the range [-1,1]. The final auditory code is a (24×\n9) 216 dimension vector for each utterance. An example auditory feature vector is shown\nin Figure 2 (bottom).\n","The visual data were processed using software designed and written by Ramprasad Polana\n[4]. Visual frames were digitized as 64× 64 8 bit gray-level images using the Datacube\nMaxVideo system. Segments were taken as 6 frames before the acoustically determined\nutterance offset and 4 after. The normal flow was computed using differential techniques\nbetween successive frames. Each pair of frames was then averaged and then these averaged\nframes were divided into 25 equal areas (5× 5) and the motion magnitudes within each\nframe were averaged within each area. The final visual feature vector of dimension (5\nframes × 25 areas) 125 was linearly normalized as for the auditory vectors. An example\nvisual feature vector is shown in Figure 2 (top).\n","The original auditory and visual feature vectors were divided into two parts (called Ax, Ay\nand Vx,Vy as shown in Figure 2). The partition was arbitrarily determined as a compromise\nbetween wanting a similar number of dimensions and similar information content in each\npart. (We did not search over partitions; the experiments below were performed only for\nthis partition). Our goal is to combine them in different ways and observe the performance\nof the minimizing-disagreement algorithm.\n"],"Page":3,"Number":2,"Type":"Figure","CaptionBB":[173,318,675,348],"Height":1100,"Width":850,"DPI":100,"ImageBB":[297,110,553,287]}