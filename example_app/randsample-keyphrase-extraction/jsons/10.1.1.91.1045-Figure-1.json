{"Caption":"Figure 1: The three possible strategy dynamics of GIGA in self-play in two-player, two-action games. (c) corresponds to the subclass of games where GIGA\u2019s strategies do not converge. ","ImageText":[{"Text":"D","TextBB":[327.968,181.643,338.419,199.086],"Rotation":0},{"Text":"D","TextBB":[574.4,126.789,584.887,144.291],"Rotation":0},{"Text":"A","TextBB":[704.515,123.763,715.002,141.265],"Rotation":0},{"Text":"C","TextBB":[577.426,256.904,587.114,274.406],"Rotation":0},{"Text":"B","TextBB":[704.515,253.878,714.203,271.38],"Rotation":0},{"Text":"B","TextBB":[512.524,198.531,522.179,215.973],"Rotation":0},{"Text":"C","TextBB":[412.405,281.159,422.06,298.601],"Rotation":0},{"Text":"(b)","TextBB":[416.936,302.156,433.071,314.61],"Rotation":0},{"Text":"(a)","TextBB":[198.124,302.156,213.483,314.61],"Rotation":0},{"Text":"(c)","TextBB":[636.523,302.156,651.883,314.61],"Rotation":0}],"Mention":["Gradient ascent is a simple and common technique for finding parameters that optimize a target function. In the case\nof learning in games, the parameters represent the player\u2019s strategy, and the target function is expected reward. We\nwill examine three recent results evaluating gradient ascent learning algorithms in normal-form games.\nSingh, Kearns, and Mansour (2000) analyzed gradient ascent in two-player, two-action games, e.g., Table 1(a) and\n(b). They derived the gradient of expected value with respect to a player\u2019s strategy and proposed updating a player\u2019s\nstrategy in the direction of increasing gradient. The algorithm is often called IGA (Infinitesimal Gradient Ascent) due\nto their method of analysis. They examined the resulting strategy trajectories and payoffs in self-play, focusing on the\ncontinuous-time, infinitesimal step, version of the algorithm. The resulting differential equations form a two-variable\naffine dynamical system. These systems are well understood and are known to have at most three qualitative forms.\nThe strategy trajectories in these three forms are shown in Figure 1. In forms (a) and (b), they showed the strategies\nwill converge to a Nash equilibrium. In case (c), of which Table 1(a) and (b) are examples, they showed that the\nstrategies do not converge, but instead orbit the only equilibrium in the game. They proved, instead, that the average\npayoffs (a weaker form of convergence) converge to the payoffs of the Nash equilibrium.\nWoLF-IGA (2002) extended the work by Singh and colleagues to a stronger form of convergence, namely conver-\ngence of strategies. They introduced a variable learning rate, so players would change the size of steps taken in the\ndirection of the gradient. Using the WoLF (\u201CWin or Learn Fast\u201D) principle, the algorithm would choose a larger step\n","amounts to a step of size 2Î·\/3. Therefore, GIGA-WoLF is simply a variable learning rate added to IGA, allowing us\nto make use of much of the WoLF-IGA proof (Bowling & Veloso, 2002) (see Lemmas 3, 4, and 5). The only part\nof that proof that does not immediately hold for GIGA-WoLF is the case when the game\u2019s corresponding dynamical\nsystem has imaginary eigenvalues and the center of the dynamics is strictly inside the unit square. We now consider\nthis subcase in detail.\nGIGA\u2019s (i.e., IGA\u2019s) dynamics for this case are shown in Figure 1(c). The only Nash equilibrium is the center, and\n","Two-Player, Two-Action Games. We begin by exploring GIGA-WoLF and GIGA in self-play in two-player, two-\naction games. We specifically focus on games that fall into case (c) in Figure 1 in order to explore Theorem 2 in\npractice. Figure 3 shows the strategy trajectories in both Matching Pennies (top), and the Tricky Game (bottom).\nNotice that GIGA\u2019s policies follow orbits around the equilibrium, (1\/2, 1\/2) in both games. GIGA-WoLF\u2019s strategies,\non the other hand, spiral toward the equilibrium as suggested by Theorem 2. We don\u2019t show results of the strategies\nwhen GIGA-WoLF plays against GIGA, as the trajectories are nearly identical to those shown.\n"],"Page":5,"Number":1,"Type":"Figure","CaptionBB":[98,330,750,361],"Height":1100,"Width":850,"DPI":100,"ImageBB":[106,99,743,316]}