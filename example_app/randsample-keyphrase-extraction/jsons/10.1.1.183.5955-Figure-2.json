{"Caption":"Figure 2: A diagram of the network topology of the Cuernavaca network. The arrows represent the direction of ﬁle transfers and are marked with the mean eﬀective bandwidth, the mean number of re- tries and the number of samples. Node K did not transfer any ﬁles over the period of collected ﬁles. Not all the transfers for the collected data are shown. Figure 3 shows the range and variability of the bandwidth and retries of characteristic links. ","ImageText":[],"Mention":["The Mesoamerican Subduction Experiment (MASE) broad-\nband seismic array [3] is a challenged network. MASE con-\nsists of 100 seismic stations stretching 500 KM from Aca-\npulco to Tampico via Mexico city. Of these 100 stations,\n50 are stand-alone data-logger systems, while 50 are part\nof an experimental networked sensing system. The net-\nworked nodes are based on the Stargate [10] platform and are\nnetworked to peers over 5-10 Km distances using hi-power\n802.11B cards and directional antennae. In some cases, the\nbest network topology reﬂects the physical topology, and\nthe result is a tree like conﬁguration. In other cases, the\nnetwork topology is more complex, particularly when trade\noﬀs were made for a good sensor location. In all cases, relay\nnodes were required.\nBecause of poor links and other disruptions, end-to-end\nperformance in this network falls oﬀ rapidly as the number\nof hops increases. This has led us to use DTN techniques\nfor data transfer rather than multiple parallel end to end\nconnections. The sensor data is buﬀered, stored into bun-\ndles, and transferred hop by hop until it reaches a sink node.\nOur implementation of this technique only changes the way\na data delivery tool operates and not the underlying network\nservices: we use TCP to transfer the data bundles between\nhops. In addition to delivering the data, we add meta-data\nto the bundles as they are transferred between links to track\nthe movement of the data and to collect information about\nthe individual links.\nSystem management beyond the ﬁrst few hops into the\nnetwork becomes diﬃcult as end to end connections become\nextremely high latency and unreliable. The goal with sys-\ntem management is to perform a management task on all\nthe nodes in the network or to query system information\nfrom all the nodes in the network without disrupting the\ndata movement. To accomplish this, we adapted an existing\nmanagement tool, the remote shell, by changing the way it\nfundamentally operates. We pair this new type of shell with\na new underlying network service call StateSync. StateSync\nis a reliable and eﬃcient publish-subscribe mechanism that\nprovides a low latency transport for state dissemination sim-\nilar to DTN. The result of the combination is the Disruption\nTolerant Shell (DTS).\nDTS uses StateSync to reliably disseminate shell com-\nmands and scripts and to return their results. DTS speciﬁ-\ncally addresses the situations where end to end connections\nfail at critical times, are intermittent, or are just not pos-\nsible. DTS provides a tractable management environment:\nit enables the user to issue commands once and be certain\nthat all nodes will execute them, whenever and however they\nmanage to get connected. The majority of the time, DTS\nwill have lower latency than an end to end management sys-\ntem, including the cases where the end to end systems fail\nto establish and sustain connections. The remainder of the\ntime DTS will have comparable latency to and end to end\nsystem.\nDTS is currently deployed on a 13 node network that be-\ngins in Cuernavaca, as shown in Figure 1. Section 2 covers\nrelated work. Section 3 provides some analysis of the de-\nployed network. Section 4 describes the implementation of\nDTS. Section 5 discusses our evaluation of DTS.\n","ﬁll up and eventually data will be lost. When such links\nare detected, someone must physically visit the stations and\nattempt to ﬁx the situation. The characterization is also\nimportant because it can provide clues as to how well other\nsoftware might perform on the network.\nThe collected transfer times between the nodes can be\nused to determine the estimated bandwidth for each link.\nThe estimated bandwidth for each link is what provides our\ncharacterization of the network and can be used to evaluate\nthe eﬀectiveness of the network. To determine the eﬀec-\ntiveness we can compare the estimated bandwidth to the\nrequired bandwidth for any node along any given path. For\ninstance, consider a node at the edge of the network without\nany other nodes sending data through it. Given that each\nnode generates 24 ﬁles a day with each ﬁle approximately\n1.5MB, the minimum required sustained bandwidth would\nbe about 416 bytes per second. The minimum required sus-\ntained bandwidth will be greater for nodes that are closer\nto the sink, since they will be along the path to the sink for\nother nodes.\nEach of the sub networks has one station that is connected\nto the internet, so all the nodes send the bundles to this sink\nnode. To determine the path to the sink, we build a sink\ntree based on the ETX path metric [5]. To reliably dissem-\ninate the path metric information and build the paths we\nuse the StateSync mechanism which is a reliable and eﬃ-\ncient publish-subscribe mechanism described in Section 4.5.\nEach node uses StateSync to publish information over one\nhop including the sink\u2019s ID and the publishing nodes full\nchosen path and ETX to the sink.\nThe thirteen-node network beginning in Cuernavaca and\nheading south contains the stations shown in Table 1. Some\nof the information collected from the transfer information\nadded to the bundles is represented in Figure 2. Each ar-\nrow represents the movement of bundles, with the source\npointing towards the destination. Each arrow also shows\nthe mean eﬀective bandwidth, the mean number of retries,\nand the total number of ﬁles transferred along that hop.\nThe eﬀective bandwidth is the bandwidth computed using\nthe ﬁle size over the total time it takes to transfer the ﬁle\nincluding resumes and the time between retries. The data\n","shows that the bundles take multiple paths to the sink. This\nindicates that the links are variable enough to cause the path\nto the sink to change. The link variability is most likely due\nto environmental conditions.\nFigure 3 shows the distribution of eﬀective bandwidth\nmeasurements for four representative nodes and the num-\nber of other hops that have similar distributions as well as\nthe percentage of retries for the representative hops. This\ndata supports, as does Figure 2, that over the entire net-\nwork, the bandwidth varies greatly from hop to hop. The\nhops that have a larger overall bandwidth have a large vari-\nability over that bandwidth. This indicates the instability\nof the links and that performance varies over time. This is\nmost likely due to the long distances of the links and the\nenvironmental conditions.\n"],"Page":3,"Number":2,"Type":"Figure","CaptionBB":[73,611,407,740],"Height":1100,"Width":850,"DPI":100,"ImageBB":[79,385,403,596]}