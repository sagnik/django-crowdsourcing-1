{"Caption":"Figure 5: Memory availability on PlanetLab","ImageText":[{"Text":"80","TextBB":[120.786,120.389,127.598,134.14],"Rotation":0},{"Text":"Available","TextBB":[100.788,163.506,110.894,197.304],"Rotation":3},{"Text":"MB","TextBB":[100.788,148.69,110.894,161.189],"Rotation":3},{"Text":"Memory","TextBB":[100.788,116.288,110.894,146.373],"Rotation":3},{"Text":"100","TextBB":[117.381,100.98,127.598,114.731],"Rotation":0},{"Text":"60","TextBB":[120.786,139.798,127.597,153.548],"Rotation":0},{"Text":"40","TextBB":[120.786,159.207,127.596,172.957],"Rotation":0},{"Text":"20","TextBB":[120.785,178.617,127.595,192.367],"Rotation":0},{"Text":"0","TextBB":[124.19,198.023,127.596,211.774],"Rotation":0},{"Text":"05\/Dec","TextBB":[140.035,211.158,159.439,224.909],"Rotation":0},{"Text":"Min","TextBB":[138.209,231.973,148.077,245.724],"Rotation":0},{"Text":"06\/Jan","TextBB":[197.42,211.155,215.808,224.905],"Rotation":0},{"Text":"1st","TextBB":[193.465,231.97,201.636,245.721],"Rotation":0},{"Text":"Q","TextBB":[203.339,231.97,208.104,245.721],"Rotation":0},{"Text":"06\/Feb","TextBB":[253.912,211.151,272.98,224.902],"Rotation":0},{"Text":"Median","TextBB":[248.049,231.966,268.134,245.717],"Rotation":0},{"Text":"06\/Mar","TextBB":[303.574,211.148,322.636,224.899],"Rotation":0},{"Text":"3rd","TextBB":[312.844,231.963,321.695,245.714],"Rotation":0},{"Text":"Q","TextBB":[323.398,231.963,328.164,245.714],"Rotation":0},{"Text":"06\/Apr","TextBB":[360.874,211.145,378.919,224.896],"Rotation":0},{"Text":"Max","TextBB":[376.623,231.96,388.194,245.711],"Rotation":0}],"Mention":["how much physical memory individual VMs were con-\nsuming when they were reset between November 2004\nand April 2006. We note that about 10% of the resets\n(corresponding largely to the first 10% of the distribution)\noccurred on nodes with less than 1GB memory, where\nmemory pressure was tighter. Over 80% of all resets had\nbeen allocated at least 128MB. Half of all resets occurred\nwhen the slice was using more than 400MB of memory,\nwhich on a shared platform like PlanetLab indicates ei-\nther a memory leak or poor experiment design (e.g., a\nlarge in-memory logfile).\nFigure 5 shows CoMon\u2019s estimate of how many MB of\nmemory are available on each PlanetLab node. CoMon\nestimates available memory by allocating 100MB, touch-\ning random pages periodically, and then observing the\nsize of the in-memory working set over time. This serves\nas a gauge of memory pressure, since if physical memory\nis exhausted and another slice allocates memory, these\npages would be swapped out. The CoMon data shows\nthat a slice can keep a 100MB working set in memory\non at least 75% of the nodes (since only the minimum\nand first quartile line are really visible), so it appears that\nthere is not as much memory pressure on PlanetLab as we\nexpected. This also reinforces our intuition that pl mom\nresets slices mainly on nodes with too little memory or\nwhen the slice\u2019s application has a memory leak.\n"],"Page":10,"Number":5,"Type":"Figure","CaptionBB":[158,258,355,275],"Height":1100,"Width":850,"DPI":100,"ImageBB":[101,101,416,246]}