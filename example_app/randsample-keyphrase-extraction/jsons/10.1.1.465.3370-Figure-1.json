{"Caption":"Figure 1: Scene together with the scanning equipment on the left ","ImageText":[],"Mention":["In our scenario free-form objects in an office environment\nhave to be identified. In the current version, we mainly\nfocus on the concept of the supporting plane. When the\nfunction of an object part is to support a potential other ob-\nject, this part has to be parallel to the ground. A full three-\ndimensional segmentation based approach is not necessary\nwhen additional clues like object arrangement information is\ngiven by the user. In the future, we will augment the system\nwith more refined 3D reconstruction abilities. The three-\ndimensional surface points resulting from the laser range\nimage are separated into up to four layers. We then project\neach layer into a two-dimensional plane. Within this plane\nwe now can robustly segment object parts by using stan-\ndard methods. These segments, representing object parts in\ncertain heights, are then used to identify the whole three-\ndimensional object. The approach performs best for objects\nhaving strong functional constraints at the system\u2019s current\nperceptual granularity (e.g. desks, tables, chairs, open doors,\nand empty book shelfs). However, smaller objects on the\nground (e.g. waste paper baskets, briefcases etc.) can be\ndetected but not classified reliably by our current system.\nThese objects can however be referred to by a human, and\nfurthermore they can be referred to by reference to other\nobjects in the environment (e.g. \u201Dthe briefcase behind the\nchair\u201D). In the next section we present a model of projective\nrelations which can be used by a robotic agent to facilitate\nreference to these items which could not otherwise be cate-\ngorized.\nFigure 1 shows an experimental scene together with the\nscanning equipment. The scene consists of two chairs, a\nwaste paper basket and a briefcase. Figure 2 (b) shows the\nresulting segments of the lowest level.\n","In this section first we look at how our projective relations\nmodel was combined with our object recognition system, to\nfacilitate natural, linguistic interaction between human and\nmachine. In our system, users interact by verbally issuing\nsimple requests to the system. These requests - to identify\nitems in the system\u2019s perceptual range - are detected with a\nNuance Speech Recognizer3 , before being fed to a semantic\nanalysis component. This analysis attempts to identify the\ncategory of the object to be identified, the referent object,\nand the spatial relationship employed by the user to relate\nthe referent to the target object.\nA projection of the recognized 3D objects onto the plane\nproduces a 2D map, defined in terms of object location\nfor directed and undirected objects, object categorization (if\navailable), and camera position and angle. This map is used\nas input for our integration module; the module - based on\nthe spatial projections model presented in section - gets the\nspatial knowledge expression\/proposition from the semantic\nanalysis component, and attempts to identify the target ob-\nject in the 2Dmap using the projective relations defined. The\nmost probable target object, once computed, is then high-\nlighted.\nThe system\u2019s results can be illustrated using an example\nscene (see figure 1): Figure 8 shows two acceptance regions\ngenerated based on the request \u201Cshow me the briefcase in\nfront of the chair\u201D. The two acceptance regions for relative\nreference are also evaluated. The correct referent (the brief-\ncase, see figures 1 and 8) is selected because it has the high-\nest confidence value. The ranking of the interpretations for\nour sample configuration is the following. The briefcase in a\nrelative reference system using the left chair as relatum gets\nthe highest confidence value (0.98), the intrinsic interpreta-\ntion yields a confidence value of 0.81. And the bin between\nboth chairs gets a confidence value of 0.08 for a relative ref-\nerence interpretation with respect to the chair on the left.\n"],"Page":2,"Number":1,"Type":"Figure","CaptionBB":[73,834,406,867],"Height":1100,"Width":850,"DPI":100,"ImageBB":[80,580,401,822]}