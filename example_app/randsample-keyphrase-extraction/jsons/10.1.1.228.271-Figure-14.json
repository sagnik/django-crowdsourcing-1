{"Caption":"       Number of Concurrent Clients Figure 14: Concurrent Querie ","ImageText":[{"Text":"Total","TextBB":[95.5531,198.856,104.556,219.725],"Rotation":3},{"Text":"Time","TextBB":[95.5531,175.386,104.556,196.246],"Rotation":3},{"Text":"for","TextBB":[95.5531,161.821,104.556,172.776],"Rotation":3},{"Text":"200","TextBB":[95.5531,143.552,104.556,159.211],"Rotation":3},{"Text":"Queries","TextBB":[95.5531,108.076,104.556,140.942],"Rotation":3},{"Text":"(secs)","TextBB":[95.5531,79.9124,104.556,105.466],"Rotation":3},{"Text":"300","TextBB":[108.128,82.1605,123.787,91.1634],"Rotation":0},{"Text":"100","TextBB":[108.128,95.7728,123.787,104.776],"Rotation":0},{"Text":"10","TextBB":[113.348,124.272,123.787,133.275],"Rotation":0},{"Text":"Scan","TextBB":[197.343,151.63,218.738,160.633],"Rotation":0},{"Text":"Cracking","TextBB":[181.703,162.896,218.738,171.899],"Rotation":0},{"Text":"Empty","TextBB":[142.042,174.161,168.647,183.164],"Rotation":0},{"Text":"Table","TextBB":[171.257,174.161,194.736,183.164],"Rotation":0},{"Text":"Scan","TextBB":[197.346,174.161,218.741,183.164],"Rotation":0},{"Text":"1","TextBB":[118.567,152.77,123.787,161.773],"Rotation":0},{"Text":"0.1","TextBB":[110.738,181.336,123.787,190.339],"Rotation":0},{"Text":"0.01","TextBB":[105.519,209.835,123.788,218.837],"Rotation":0},{"Text":"1","TextBB":[133.009,219.222,138.229,228.225],"Rotation":0},{"Text":"2","TextBB":[165.062,219.222,170.281,228.225],"Rotation":0},{"Text":"4","TextBB":[197.181,219.222,202.401,228.225],"Rotation":0},{"Text":"8","TextBB":[226.691,219.222,231.91,228.225],"Rotation":0},{"Text":"10","TextBB":[237.019,219.222,247.458,228.225],"Rotation":0},{"Text":"20","TextBB":[265.156,219.222,275.596,228.225],"Rotation":0},{"Text":"25","TextBB":[280.701,219.222,291.141,228.225],"Rotation":0},{"Text":"50","TextBB":[311.517,219.222,321.957,228.225],"Rotation":0},{"Text":"100","TextBB":[341.026,219.222,356.685,228.225],"Rotation":0},{"Text":"200","TextBB":[373.078,219.222,388.737,228.225],"Rotation":0}],"Mention":["Concurrency control, logging, & recovery. We are currently\nexploring how to best implement concurrency control, logging, and\nrecovery in an adaptive indexing system.\nAs we have already shown in [4], distinguishing between database\ncontents and representation has significant implications for concur-\nrency control. Insofar as read queries performing scans or index\nlookups can invoke side-effect operations that incrementally refine\nthe physical design, \u201Cread\u201D queries may seem to introduce signifi-\ncant concurrency control overhead as they refine index structures.\nHowever, we observe that the adaptive indexing operations impact\nonly index structures, and never database contents. Furthermore,\nindex refinement is optional and can be done opportunistically or\neven skipped altogether. These distinctions relax constraints and re-\nquirements with regard to concurrency control of adaptive indexing\ncompared to those of traditional explicit index updates and enable\nnew techniques for reducing the performance overhead of concur-\nrency control during structural updates.\nHere we give an example of the performance of adaptive index-\ning under concurrent queries. For this example we use database\ncracking. We made sure that all critical structures are protected\nvia mutexes, using a very basic approach with per-column granu-\nlarity. We use a table of 100 million tuples with unique randomly\ndistributed integers and simple range queries of the form select\ncount(*) from R where v1 < A < v2 .\nWe repeatedly run a sequence of 200 random range queries (10%\nselectivity), each time increasing the number of concurrent streams.\nIn more detail, we run the serial case where one client runs all 200\nqueries, one after the other. Then, we use 2 clients that start at the\nsame time and each one fires 100 queries. Then, we repeat the ex-\nperiment by starting 4 clients at the same time and each one fires 50\nqueries and so on. The last run uses 200 clients that simultaneously\nfire 1 query each. To ensure that our results are not dominated by\nmanagement overhead for handling up to 200 concurrent clients,\nwe first run our query over an empty table, i.e., with pure query\nexecution time being virtually zero.\nFigure 14 depicts the results. Database cracking maintains a\nrather stable performance similar to the scan based approach. Note\nthat with scan, there is no concurrency control mechanisms in-\nvolved as this is purely a read only workload. Performance im-\nproves initially due to temporal locality of accessing data across\nqueries and then as we reach the limits of the hardware with more\nconcurrent clients it increases again. The main point here is that\nthere is no significant degradation in adaptive indexing. If we mea-\nsure the pure locking cost of the adaptive index, i.e., the cost spent\nin acquiring and releasing locks it is less than one second. Thus,\nby using short latching periods and quickly releasing latches as\nsoon as possible, database cracking manages to exploit concurrent\nqueries as opposed to suffering from them. In addition, it is in-\nteresting to notice that since cracking gains continuously more and\n"],"Page":12,"Number":14,"Type":"Figure","CaptionBB":[155,231,319,257],"Height":1100,"Width":850,"DPI":100,"ImageBB":[93,78,391,229]}