{"Caption":"Figure 4: Peer-asymmetry of await for ddr workload with disk-hog fault. ","ImageText":[{"Text":"400","TextBB":[483.752,390.705,491.621,401.306],"Rotation":3},{"Text":"600","TextBB":[483.752,363.179,491.621,373.78],"Rotation":3},{"Text":"800","TextBB":[483.752,335.653,491.621,346.254],"Rotation":3},{"Text":"Faulty","TextBB":[658.366,332.284,675.355,340.153],"Rotation":0},{"Text":"server","TextBB":[677.122,332.284,694.81,340.153],"Rotation":0},{"Text":"Nonâˆ’faulty","TextBB":[658.366,339.911,688.734,347.78],"Rotation":0},{"Text":"servers","TextBB":[690.501,339.911,711.367,347.78],"Rotation":0},{"Text":"Peer-asymmetry","TextBB":[554.819,395.43,624.615,407.233],"Rotation":0},{"Text":"200","TextBB":[483.752,418.231,491.621,428.833],"Rotation":3},{"Text":"0","TextBB":[483.752,449.287,491.621,452.82],"Rotation":3},{"Text":"Request","TextBB":[468.498,401.307,476.367,424.977],"Rotation":3},{"Text":"Wait","TextBB":[468.498,387.083,476.367,399.54],"Rotation":3},{"Text":"Time","TextBB":[468.498,371.194,476.367,385.316],"Rotation":3},{"Text":"(ms)","TextBB":[468.498,356.721,476.367,369.427],"Rotation":3},{"Text":"0","TextBB":[505.328,463.543,508.862,471.411],"Rotation":0},{"Text":"100","TextBB":[530.084,463.543,540.685,471.411],"Rotation":0},{"Text":"200","TextBB":[558.373,463.543,568.974,471.411],"Rotation":0},{"Text":"300","TextBB":[586.656,463.543,597.258,471.411],"Rotation":0},{"Text":"400","TextBB":[614.945,463.543,625.546,471.411],"Rotation":0},{"Text":"500","TextBB":[643.234,463.543,653.835,471.411],"Rotation":0},{"Text":"600","TextBB":[671.522,463.543,682.124,471.411],"Rotation":0},{"Text":"700","TextBB":[699.806,463.543,710.407,471.411],"Rotation":0},{"Text":"Elapsed","TextBB":[584.776,478.797,607.739,486.665],"Rotation":0},{"Text":"Time","TextBB":[609.506,478.797,623.629,486.665],"Rotation":0},{"Text":"(s)","TextBB":[625.396,478.797,632.807,486.665],"Rotation":0}],"Mention":["across each I\/O server with a default stripe size of 64 kB.\nFor each file-object, the first stripe segment is located\non the I\/O server to which the object handle is assigned.\nSubsequent segments are accessed in a round-robin man-\nner on each of the remaining I\/O servers. This character-\nistic has significant implications on PVFS\u2019s throughput\nin the event of a performance problem.\nLustre clusters consist of one active metadata server\nwhich serves one metadata target (storage space), one\nmanagement server which may be colocated with the\nmetadata server, and multiple object storage servers\nwhich serve one or more object storage targets each.\nThe metadata and object storage servers are analogous to\nPVFS\u2019s metadata and I\/O servers with the main distinc-\ntion of only allowing for a single active metadata server\nper cluster. Unlike PVFS, the Lustre server is imple-\nmented entirely in kernel space as a loadable kernel mod-\nule. The Lustre client is also implemented as a kernel\nspace file-system module, and like PVFS, provides file\nsystem access via the Linux VFS interface. A userspace\nclient library (liblustre) is also available.\nLustre allows for the configurable striping of file data\nacross one or more object storage targets. By default, file\ndata is stored on a single target. The stripe_count\nparameter may be set on a per-file, directory, or file-\nsystem basis to specify the number of object storage tar-\ngets that file data is striped over. The stripe_size\nparameter specifies the stripe unit size and may be con-\nfigured to multiples of 64 kB, with a default of 1MB (the\nmaximum payload size of a Lustre RPC).\n","For disk-busy and disk-hog faults, await, avgqu-sz\nand %util increase at the faulty server as the disk\u2019s\nresponsiveness decreases and requests start to backlog.\nThe increased await on the faulty server causes an\nincreased server response-time, making the client wait\nlonger before it can issue its next request. The additional\ndelay that the client experiences reduces its I\/O through-\nput, resulting in the fault-free servers having increased\nidle time. Thus, the await and %utilmetrics decrease\nasymmetrically on the fault-free I\/O servers, enabling a\npeer-comparison diagnosis of the disk-hog and disk-busy\nfaults, as seen in Figure 4.\n"],"Page":5,"Number":4,"Type":"Figure","CaptionBB":[436,503,751,534],"Height":1100,"Width":850,"DPI":100,"ImageBB":[467,324,721,488]}