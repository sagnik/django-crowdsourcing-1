{"Caption":"Figure 4: SQL query plan for column marginal class","ImageText":[{"Text":"Top","TextBB":[593.832,104.795,611.404,113.483],"Rotation":0},{"Text":"K","TextBB":[614.138,104.795,621.985,113.483],"Rotation":0},{"Text":"Target","TextBB":[624.815,104.795,653.118,113.483],"Rotation":0},{"Text":"Objects","TextBB":[655.949,104.795,689.202,113.483],"Rotation":0},{"Text":"Order","TextBB":[603.499,130.462,628.958,139.15],"Rotation":0},{"Text":"by","TextBB":[631.672,130.462,642.613,139.15],"Rotation":0},{"Text":"score","TextBB":[645.345,130.462,668.178,139.15],"Rotation":0},{"Text":"⋈","TextBB":[626.832,147.294,638.908,168.725],"Rotation":0},{"Text":"GROUP","TextBB":[533.499,182.962,570.354,191.65],"Rotation":0},{"Text":"BY","TextBB":[573.145,182.962,588.325,191.65],"Rotation":0},{"Text":"TOId","TextBB":[533.499,194.628,557.106,203.316],"Rotation":0},{"Text":"(F","TextBB":[559.838,194.628,569.541,203.316],"Rotation":0},{"Text":"agg","TextBB":[569.499,199.493,579.955,205.285],"Rotation":0},{"Text":")","TextBB":[579.999,194.628,583.618,203.317],"Rotation":0},{"Text":"⋈","TextBB":[522.499,213.96,534.575,235.392],"Rotation":0},{"Text":"Ranked","TextBB":[440.332,252.462,473.617,261.15],"Rotation":0},{"Text":"list","TextBB":[476.349,252.462,489.682,261.15],"Rotation":0},{"Text":"of","TextBB":[492.494,252.462,501.465,261.15],"Rotation":0},{"Text":"documents","TextBB":[440.332,263.962,487.559,272.65],"Rotation":0},{"Text":"matching","TextBB":[440.332,275.628,480.958,284.317],"Rotation":0},{"Text":"w","TextBB":[483.691,275.628,491.538,284.317],"Rotation":0},{"Text":"1","TextBB":[491.499,280.493,495.121,286.285],"Rotation":0},{"Text":"(F","TextBB":[659.832,155.795,669.374,164.483],"Rotation":0},{"Text":"comb","TextBB":[669.499,160.66,685.622,166.452],"Rotation":0},{"Text":")","TextBB":[685.665,155.795,689.284,164.483],"Rotation":0},{"Text":"TOId","TextBB":[641.665,163.195,662.663,170.917],"Rotation":0},{"Text":"GROUP","TextBB":[624.999,184.128,661.854,192.816],"Rotation":0},{"Text":"BY","TextBB":[664.645,184.128,679.825,192.816],"Rotation":0},{"Text":"TOId","TextBB":[624.999,195.794,648.606,204.483],"Rotation":0},{"Text":"(F","TextBB":[651.338,195.794,661.041,204.483],"Rotation":0},{"Text":"agg","TextBB":[660.999,200.66,671.622,206.452],"Rotation":0},{"Text":")","TextBB":[671.499,195.795,675.118,204.483],"Rotation":0},{"Text":"⋈","TextBB":[667.165,215.793,679.241,237.225],"Rotation":0},{"Text":"DocId","TextBB":[534.665,229.695,558.844,237.417],"Rotation":0},{"Text":"Relationships","TextBB":[531.499,252.462,590.893,261.15],"Rotation":0},{"Text":"Table","TextBB":[531.499,263.962,556.307,272.65],"Rotation":0},{"Text":"R","TextBB":[558.998,263.962,565.639,272.65],"Rotation":0},{"Text":"GROUP","TextBB":[694.498,182.962,731.541,191.65],"Rotation":0},{"Text":"BY","TextBB":[734.166,182.962,749.346,191.65],"Rotation":0},{"Text":"TOId","TextBB":[694.498,194.628,718.099,203.316],"Rotation":0},{"Text":"(F","TextBB":[720.998,194.628,730.54,203.316],"Rotation":0},{"Text":"agg","TextBB":[730.665,199.493,741.122,205.285],"Rotation":0},{"Text":")","TextBB":[741.165,194.628,744.784,203.317],"Rotation":0},{"Text":"DocId","TextBB":[679.165,231.695,703.496,239.417],"Rotation":0},{"Text":"Ranked","TextBB":[605.165,252.462,638.407,261.15],"Rotation":0},{"Text":"list","TextBB":[641.14,252.462,654.501,261.15],"Rotation":0},{"Text":"of","TextBB":[657.149,252.462,666.268,261.15],"Rotation":0},{"Text":"documents","TextBB":[605.165,263.962,652.42,272.65],"Rotation":0},{"Text":"matching","TextBB":[605.165,275.628,645.621,284.317],"Rotation":0},{"Text":"w","TextBB":[648.52,275.628,656.367,284.317],"Rotation":0},{"Text":"2","TextBB":[656.332,280.493,659.955,286.285],"Rotation":0},{"Text":"\u2026","TextBB":[735.165,222.902,744.045,234.544],"Rotation":0},{"Text":"Relationships","TextBB":[690.832,252.462,750.059,261.15],"Rotation":0},{"Text":"Table","TextBB":[690.832,263.962,715.648,272.65],"Rotation":0},{"Text":"R","TextBB":[718.331,263.962,724.972,272.65],"Rotation":0}],"Mention":["keywords and how well they match with those keywords. Thus, the \nrelevance score of an entity is an \u201Caggregate\u201D of the FTS scores \n(i.e., the keyword match scores returned by FTS) of all related \nreviews containing the query keywords. For example, the entity \n\u2018Dell Inspiron 700m\u2019 in Figure 1 is related to two reviews that \ncontain keyword \u201Clightweight\u201D (d1 and d3) and two reviews that \ncontain keyword \u201Cbusiness use\u201D (d3 and d6). So, the relevance \nscore \u2018Dell Inspiron 700m\u2019 for those keywords is obtained by \naggregating the FTS scores of d1 and d3 for keyword \u201Clightweight\u201D \nand those of d3 and d6 for keyword \u201Cbusiness use\u201D. In this paper, \nwe consider a broad class of scoring functions to compute \nrelevance scores of target objects as aggregations over FTS scores \nof the related documents. Our OF query evaluation system would \nthen return the K target objects with the best scores according to \nany chosen scoring function from the above class. Informally, the \nproblem is to compute the top K target objects with the highest \nscores, obtained by aggregating over one or more ranked lists. The \ntechniques we describe in this paper are applicable to this general \nclass of queries, where ranked lists are obtained using any indexing \nsub-system (not necessarily FTS). However, while describing \ntechniques in this paper, we assume that search objects are \ndocuments and the indexing sub-system is FTS, which returns a \nranked list of documents for keyword queries.  \nThe requirement that we aggregate (multiple) ranked lists of \nobject scores prevents us from using existing work, especially the \nTA family of algorithms [10,11]. The threshold algorithm (TA) \nassumes that an object has a single score in each list. In our case, a \ntarget object can have multiple document scores, which need to be \naggregated, in each list. (See Section 2 for a detailed discussion). \nMost relational DBMSs now support FTS functionality. \nHence, OF queries can be implemented in SQL. Figure 4 illustrates \nthe schematic of such a query plan. SQL evaluations performing \naggregation over FTS scores would be forced to retrieve all the \ndocuments containing the set of query keywords, join them all with \nthe relationships table to find the related target objects, compute the \naggregate scores of all these target objects, sort them all based on \nthe score, and return the top K to the user. For large document \ncollections, FTS may return large numbers of documents causing \nthis implementation to be very inefficient. The challenge is to \nexploit the property that we only require the top K target objects \nand terminate early.  \n","Commercial DBMSs now support FTS functionality by \nproviding specialized user-defined functions (UDFs) to perform \nkeyword search on text columns of database tables [9]. Therefore, \nwe can implement OF queries in SQL using these FTS UDFs.  \n Figure 4 shows the execution plan for the column-marginal \nclass. We join each list individually with the relationships table R \non DocId to get the related target objects. We then group each join \n","condition due to the weak upper bounds. The Generate-Prune \napproach, on the other hand, is robust due to the relaxed stopping \ncondition. Note that the Generate-Prune approach has the \nadditional cost of computing exact scores of candidates but that \ncost is small compared to the difference of cost in the generation \nphase.  \nComparison between Exact Scores and Approximate Scores: \nRecall that the Generate-Prune technique returning the top K \nobjects with approximate scores is expected to reduce cost in 2 \nways: (a) compute exact scores of fewer candidate target objects \nand (b) retrieve fewer documents from the lists. Figure 13 shows \nthe savings due to (a); the approximate scores approach compute \nexact scores of fewer candidates (by almost 25-50%). Figure 12 \nshows the savings due to (b); the approximate scores approach \nretrieves much fewer documents compared to exact scores which \nretrieve all documents (but do not lookup in the relationships table). \nHowever, the surprising result was that their execution times as \nshown in Figure 9 are almost identical. Investigating this anomaly, \nwe found that the SQL UDF we are using to get the ranked lists \nfrom FTS for the various keywords actually gets the whole ranked \nlist in one go. We confirmed this by varying the number of \ndocuments retrieved from FTS for various keyword queries and \nmeasuring the response times; the execution times are independent \nof the number of documents retrieved. Hence, the savings in the \ncost due to (b) is not reflected in the execution time. Furthermore, \nwe found that retrieving the ranked list from FTS accounts for \nabout half the execution time; the remaining time is evenly split \nbetween the generation and pruning phases. Therefore, in an FTS \nwhich does not retrieve the whole ranked lists in one go, we expect \nthe approximate scores approach to be even better compared to all \nthe other approaches including SQL.  \nSensitivity to materialization: Figure 14 shows the execution \ntimes of the Gen-Prune approach for various values of θ. Lower the \nvalue of θ, more the number of frequent target objects materialized, \nbetter the upper bound scores, faster the execution. \n"],"Page":5,"Number":4,"Type":"Figure","CaptionBB":[456,294,738,307],"Height":1100,"Width":850,"DPI":100,"ImageBB":[433,102,757,289]}