{"Caption":"Figure 6: Blocks world with the on(A, B) goal","ImageText":[],"Mention":["These experiments show that the number of episodes\nneeded to obtain a good policy is much smaller when look-\ning ahead. However, learning and lookahead takes additional\ncomputation time. Therefore, for worlds where performing\nactions is not expensive, it is also useful to evaluate how the\ntotal time needed to obtain a good policy compares with and\nwithout learning. Figure 6 shows the average reward in func-\ntion of the time needed to learn the policy for the on(a,b)\ngoal (the other settings produce similar results, omitted due\nto lack of space). Due to the computational costs of learn-\ning the model, using no lookahead performs better than sin-\ngle step lookahead. Indeed, in our current implementation,\nlearning the model of the world is the computational bottle-\nneck for small levels of lookahead. This is mainly due to the\nsampling techniques to create the learning examples, some-\nthing we will improve in future work. However, performing\ntwo steps of lookahead still outperforms the standard version\nwithout lookahead.\n"],"Page":5,"Number":6,"Type":"Figure","CaptionBB":[111,621,375,638],"Height":1100,"Width":850,"DPI":100,"ImageBB":[82,354,401,614]}