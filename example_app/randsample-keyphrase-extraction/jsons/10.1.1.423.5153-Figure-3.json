{"Caption":"Figure 3: An RL agent observes the initial state of its en- vironment, chooses an action to perform, observes the new state, and uses this information to adjust its policy. Then it chooses another action, and this cycle continues. ","ImageText":[{"Text":"observe","TextBB":[211.736,90.1298,249.212,100.066],"Rotation":0},{"Text":"state","TextBB":[218.867,104.043,242.032,113.979],"Rotation":0},{"Text":"choose","TextBB":[213.707,148.101,247.19,158.037],"Rotation":0},{"Text":"action","TextBB":[215.852,162.014,245.073,171.95],"Rotation":0},{"Text":"update","TextBB":[167.426,206.091,200.666,216.027],"Rotation":0},{"Text":"policy","TextBB":[170.209,220.004,197.878,229.941],"Rotation":0},{"Text":"observe","TextBB":[269.707,206.091,307.183,216.027],"Rotation":0},{"Text":"state","TextBB":[276.837,220.004,300.003,229.941],"Rotation":0}],"Mention":["RL is a method by which an agent can learn to act au-\ntonomously in its environment. RL algorithms typically\nperform online learning; agents incrementally update their\nknowledge as they collect information by trial-and-error.\nThe main challenge in this type of learning is to correctly as-\nsociate actions with their effects, even though some effects\nare typically delayed.\nLearning with RL often takes place in episodes. At each\nstep of an episode, an agent observes the state of its environ-\nment and chooses an action to perform. It then observes the\nresulting state and receives a reward for its action, and uses\nthis information to update its method of choosing actions,\nwhich is called a policy. The agent\u2019s goal is to learn a policy\nthat maximizes its total episode reward. Figure 3 illustrates\nthis general RL procedure.\nOne effective type of RL is Q-learning (Sutton and Barto\n1998). In a Q-learning algorithm, an agent\u2019s policy is to\ntake the action that has the highest Q-value in the current\nstate. Learning therefore reduces to assigning a Q-value to\neach state-action pair. A Q-value Q(s, a) estimates the to-\ntal episode reward achievable by taking action a in state s\nand continuing to follow the policy thereafter. Typically Q-\nvalues begin at zero, and are adjusted after each step:\n"],"Page":3,"Number":3,"Type":"Figure","CaptionBB":[73,254,406,314],"Height":1100,"Width":850,"DPI":100,"ImageBB":[143,78,329,240]}